{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Only take the first 1250 rows in the dataframe\n",
    "RESTRICT_ROWS = 1250\n",
    "RESTRICT_ROWS = None\n",
    "\n",
    "# Replicates Java's hashCode function perfectly\n",
    "def intify(i):\n",
    "  # Python's int data type by default stores an unbounded amount of data, so we trim it to a signed 32-bit integer, which is used in Java's hashCode function\n",
    "  return (i % 4294967296) - 2147483648\n",
    "def hashCode(s):\n",
    "  counter = 0\n",
    "  for i in range(len(s)):\n",
    "    counter += intify(ord(s[-i-1]) * (pow(31,i)))\n",
    "    # The iterative algorithm that computes the hash in Java\n",
    "  return intify(counter)\n",
    "\n",
    "\n",
    "def create_start_df(lines):\n",
    "    id_start = []\n",
    "    for line in lines:\n",
    "        templ = []\n",
    "        sepl = line.split(\",\")\n",
    "        timestamp = sepl[-1]\n",
    "        del sepl[-1]\n",
    "        goflow2 = json.loads(\",\".join(sepl))\n",
    "        fields = [\n",
    "            str(goflow2[\"SrcAddr\"]),\n",
    "            str(goflow2[\"DstAddr\"]),\n",
    "            str(goflow2[\"SrcPort\"]),\n",
    "            str(goflow2[\"DstPort\"]),\n",
    "            str(goflow2[\"TimeFlowStartMs\"]),\n",
    "            str(goflow2[\"TimeFlowEndMs\"])\n",
    "        ]\n",
    "        tempstr = \"-\".join(fields)\n",
    "        templ.append(str(hashCode(tempstr)))\n",
    "        templ.append(timestamp)\n",
    "        id_start.append(templ)\n",
    "\n",
    "    return pd.DataFrame(id_start, columns=[\"hash\", \"nano-start\"])\n",
    "\n",
    "\n",
    "def create_end_df(lines):\n",
    "    id_end = []\n",
    "    # process it line to make first field the hash and second field the timestamp\n",
    "    for line in lines:\n",
    "        templ = []\n",
    "        sepl = line.split(\",\")\n",
    "        timestamp = sepl[-1]\n",
    "        del sepl[-1]\n",
    "        flow_data_record = json.loads(\",\".join(sepl))[\"netflow-v9:netflow\"][\"export-packet\"][\"flow-data-record\"][0]\n",
    "        fields = [\n",
    "            str(flow_data_record[\"ipv4\"][\"src-address\"]),\n",
    "            str(flow_data_record[\"ipv4\"][\"dst-address\"]),\n",
    "            str(flow_data_record[\"src-port\"]),\n",
    "            str(flow_data_record[\"dst-port\"]),\n",
    "            str(flow_data_record[\"first-switched\"]),\n",
    "            str(flow_data_record[\"last-switched\"])\n",
    "        ]\n",
    "        tempstr = \"-\".join(fields)\n",
    "        templ.append(str(hashCode(tempstr)))\n",
    "        templ.append(timestamp)\n",
    "        id_end.append(templ)\n",
    "\n",
    "    return pd.DataFrame(id_end, columns=[\"hash\", \"nano-end\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processed files saved in: ../controlled_DATA/multiple-rates-driversapps/results_inputdriver/processed/\n"
     ]
    }
   ],
   "source": [
    "raw_path = \"../controlled_DATA/multiple-rates-driversapps/results_inputdriver/\"\n",
    "\n",
    "# Get the start and end files and create another file with the latency in milliseconds\n",
    "iterations=[1, 2, 3]\n",
    "delays=[0.016, 0.025, 0.05, 0.1, 0.2]\n",
    "\n",
    "for delay in delays:\n",
    "\n",
    "    for i in iterations:\n",
    "\n",
    "        df_start = None\n",
    "        df_end = None\n",
    "        df = None\n",
    "\n",
    "        ### READ AND PROCESS DRIVER INPUT START FILE\n",
    "        with open(raw_path+\"batch\"+str(delay)+\"_\"+str(i)+\"-start.txt\") as file:\n",
    "            start_lines = file.read().splitlines()\n",
    "\n",
    "        with open(raw_path+\"batch\"+str(delay)+\"_\"+str(i)+\"-end.txt\") as file:\n",
    "            end_lines = file.read().splitlines()\n",
    "\n",
    "        df_start = create_start_df(start_lines)\n",
    "        df_end = create_end_df(end_lines)\n",
    "\n",
    "        ### MERGE DATAFRAMES id_start and id_end\n",
    "        df = df_start.merge(df_end, left_on=['hash'], right_on=['hash'], how='outer')\n",
    "        if not df.isnull().sum().sum() == 0:\n",
    "            print(\"some hash do not coincide\")\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        if RESTRICT_ROWS is not None:\n",
    "            df = df.head(RESTRICT_ROWS)\n",
    "\n",
    "        df[\"processing_time\"] = df[\"nano-end\"].astype(float) - df[\"nano-start\"].astype(float)\n",
    "        df[\"processing_time_ms\"] = df[\"processing_time\"]/1000000\n",
    "        if not os.path.exists(raw_path+\"processed/\"):\n",
    "            os.makedirs(raw_path+\"processed/\")\n",
    "        df.to_csv(raw_path+\"processed/latency\"+str(delay)+\"_\"+str(i)+\".csv\", index=False)\n",
    "\n",
    "print(\"All processed files saved in:\", raw_path+\"processed/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processed averaged files saved in: ../controlled_DATA/multiple-rates-driversapps/results_inputdriver/processed/processed_averaged/\n"
     ]
    }
   ],
   "source": [
    "delays=[0.016, 0.025, 0.05, 0.1, 0.2]\n",
    "\n",
    "processed_path = raw_path+\"processed/\"\n",
    "\n",
    "for delay in delays:\n",
    "\n",
    "    df1 = pd.read_csv(processed_path+\"latency\"+str(delay)+\"_\"+str(1)+\".csv\")\n",
    "    df2 = pd.read_csv(processed_path+\"latency\"+str(delay)+\"_\"+str(2)+\".csv\")\n",
    "    df3 = pd.read_csv(processed_path+\"latency\"+str(delay)+\"_\"+str(3)+\".csv\")\n",
    "\n",
    "    df = df1.merge(df2[[\"hash\", \"processing_time_ms\"]], left_on=['hash'], right_on=['hash'], how='outer')\n",
    "    df = df.merge(df3[[\"hash\", \"processing_time_ms\"]], left_on=['hash'], right_on=['hash'], how='outer')\n",
    "    if not df.isnull().sum().sum() == 0:\n",
    "        print(\"some hash do not coincide\")\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df[\"processing_time_ms\"] = (df[\"processing_time_ms\"]+df[\"processing_time_ms_x\"]+df[\"processing_time_ms_y\"])/3\n",
    "    df.drop(['nano-start', 'nano-end', 'processing_time', 'processing_time_ms_x', 'processing_time_ms_y'], axis='columns', inplace=True)\n",
    "\n",
    "    if RESTRICT_ROWS is not None:\n",
    "        if len(df1) != RESTRICT_ROWS or len(df2) != RESTRICT_ROWS or len(df3) != RESTRICT_ROWS:\n",
    "            print(\"Something may be wrong with this batches\")\n",
    "\n",
    "    if not os.path.exists(processed_path+\"processed_averaged/\"):\n",
    "        os.makedirs(processed_path+\"processed_averaged/\")\n",
    "    df.to_csv(processed_path+\"processed_averaged/latency\"+str(delay)+\".csv\", index=False)\n",
    "\n",
    "print(\"All processed averaged files saved in:\", processed_path+\"processed_averaged/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dc8756dacbbaa0e55cc5ee6f7d758d6debfc7d5dfe2e1be00e102300319c58d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}